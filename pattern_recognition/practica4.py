# -*- coding: utf-8 -*-
"""practica4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5bsfERJif-5LUYjK0X_XOo7rKqpNQKX
"""

import numpy as np
import scipy
import winsound #audio de windows
import pyaudio #librería para reproducción o edición de audio 
import wave #librería para manipular archivos WAV
import sklearn
import scipy.fftpack as fourier
import matplotlib.pyplot as plt
import librosa
import librosa.display
import IPython.display as ipd
import soundfile
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from scipy.fftpack import dct
import scipy.spatial.distance as dist

from mpl_toolkits.mplot3d import Axes3D
plt.rcParams['figure.figsize'] = (14, 5)

def dtw(matriz_distancia):
    matriz_costos=np.zeros((matriz_distancia.shape[0]+1,matriz_distancia.shape[1]+1))
    
    for i in range (1,matriz_distancia.shape[0]+1):
        matriz_costos[i,0]=np.inf
    for i in range (1,matriz_distancia.shape[1]+1):
        matriz_costos[0,i]=np.inf
    for i in range (1,matriz_distancia.shape[0]):
        for j in range (1,matriz_distancia.shape[1]):
            valor=[
                    matriz_costos[i,j],
                    matriz_costos[i,j+1],
                    matriz_costos[i+1,j]]
            pos=np.argmin(valor)
            matriz_costos[i+1,j+1]=matriz_distancia[i,j]+valor[pos]
    matriz_costos=matriz_costos[i:,j:]
    return matriz_costos

def pre_emph(signal):
    alpha = 0.97 #pre emphasis
    emphasized_signal = np.append(signal[0], signal[1:] - alpha * signal[:-1])
    return emphasized_signal

def framing(emphasized_signal):
    sample_rate = 8000
    frame_size = 0.025 #25 ms frame size
    frame_stride = 0.01 #15 ms overlap, paso 10 ms
    
    frame_length,frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples
    signal_length = len(emphasized_signal)
    frame_length = int(round(frame_length))
    frame_step = int(round(frame_step))
    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame
    
    pad_signal_length = num_frames * frame_step + frame_length
    z = np.zeros((pad_signal_length - signal_length))
    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal
    
    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T
    frames = pad_signal[indices.astype(np.int32, copy=False)]
    
    #Window
    frames *= np.hamming(frame_length)
    return frames

def fourier_tf(frames):
    NFFT = 512
    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT
    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum
    return pow_frames

def filter_banks(pow_frames):
    #Mel filters
    sample_rate = 8000
    NFFT = 512
    nfilt = 40
    low_freq_mel = 0
    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel
    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale
    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz
    bin = np.floor((NFFT + 1) * hz_points / sample_rate)
    
    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))
    for m in range(1, nfilt + 1):
        f_m_minus = int(bin[m - 1])   # left
        f_m = int(bin[m])             # center
        f_m_plus = int(bin[m + 1])    # right
    
        for k in range(f_m_minus, f_m):
            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])
        for k in range(f_m, f_m_plus):
            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])
    filter_banks = np.dot(pow_frames, fbank.T)
    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability
    filter_banks = 20 * np.log10(filter_banks)  # dB
    filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8) #opcional, mejorar SNR
    return filter_banks

def MFCC(filter_banks): #Mel-frequency Cepstral Coefficients
    num_ceps = 12
    cep_lifter = 22
    mfcc = dct(filter_banks,type = 2,axis = 1,norm = 'ortho')[:,1:(num_ceps + 1)] # Keep 2-13
    
    (nframes,ncoeff) = mfcc.shape
    n = np.arange(ncoeff)
    lift = 1 + (cep_lifter/2) * np.sin(np.pi*n/cep_lifter)
    mfcc *= lift  #*
    mfcc -= (np.mean(mfcc, axis=0) + 1e-8) #opcional, mejorar SNR
    return mfcc

plt.close('all')

#Grabación
muestras = 512 # 512 muestras
formato = pyaudio.paInt16 # Resolucion de 16 bits
channels = 2 #Stereo
fs = 8000 #Frecuencia de muestreo 8000/s
seconds = 2 #Tiempo de grabacion
filename = '.wav' # Nombre del archivo

audio_obj = pyaudio.PyAudio() #objeto

input('Presionar Enter')
print('La grabación ha iniciado')

#Graba el audio
stream = audio_obj.open(format = formato,channels = channels,rate = fs,frames_per_buffer = muestras,input = True)

tramas = [] 
sonido = [] #Se lee del buffer los valores numericos

for i in range(0,int(fs/muestras*seconds)):
    datos = stream.read(muestras) #Lee muestras del stream
    tramas.append(datos)
    sonido.append(np.frombuffer(datos,dtype = np.int16))

#Detiene y cierra la grabación
stream.stop_stream()
stream.close()
audio_obj.terminate() #terminar el objeto de audio
print('La grabación ha terminado')


#Salvar el audio en formato WAV 
wf = wave.open(filename,'wb')
wf.setnchannels(channels)
wf.setsampwidth(audio_obj.get_sample_size(formato))
wf.setframerate(fs)
wf.writeframes(b''.join(tramas)) #b contenido en bytes
#wf.close()

#Reproduce audio
winsound.PlaySound(filename, winsound.SND_FILENAME | winsound.SND_ASYNC)

datos = np.hstack(sonido) 
Normalizado = datos/np.max(np.abs(datos))

plt.figure(0)
L = len(Normalizado)
t = np.arange(0,L)/fs
plt.plot(t/2,Normalizado)
plt.title('Dominio del tiempo')
plt.xlabel('Tiempo [s]')
plt.ylabel('Amplitud')

T = 1/fs
L = len(datos)
spec = np.abs(fourier.fft(datos))[:L//2]
freq = fourier.rfftfreq(L//2,T)[:L//2]
plt.figure(1)
plt.plot(freq,spec) 
plt.title('Dominio de la frecuencia')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud')

patron_g = 'cinco1.wav'
patron_a = 'palabra.wav'
nueva='cero1.wav'



x, sr = librosa.load(patron_g)
x1, sr1 = librosa.load(patron_a)
x2, sr2 = librosa.load(nueva)

X = librosa.feature.mfcc(x, sr=sr)
X1 = librosa.feature.mfcc(x1, sr=sr1)
X2 = librosa.feature.mfcc(x2, sr=sr2)


model = sklearn.decomposition.PCA(n_components=2, whiten=True)
model.fit(X.T)

Y = model.transform(X.T)
Y1 = model.transform(X1.T)
Y2 = model.transform(X2.T)

L1=np.mean(Y)
L2=np.mean(Y1)
L3=np.mean(Y2)



fig, axs = plt.subplots(3)
fig.suptitle('PCA')
axs[0].set_title('Patron grave')
axs[0].scatter(Y[:,0], Y[:,1], c='blue')
axs[1].set_title('Patron agudo')
axs[1].scatter(Y1[:,0], Y1[:,1], c='red')
axs[2].set_title('Nuevo')
axs[2].scatter(Y2[:,0], Y2[:,1],c='orange')


md=dist.cdist((Y1[:,0], Y1[:,1]),(Y[:,0], Y[:,1]),'cosine')
distancia = dtw(md)
md2=dist.cdist((Y2[:,0], Y2[:,1]),(Y[:,0], Y[:,1]),'cosine')
distancia2 = dtw(md2)
md3=dist.cdist((Y2[:,0], Y2[:,1]),(Y1[:,0], Y1[:,1]),'cosine')
distancia3 = dtw(md3)

print('Valor de distancia entre patrones: {:.4f}'.format(distancia[-1,1]))
print('Valor de distancia entre grave y nueva: {:.4f}'.format(distancia2[-1,1]))
print('Valor de distancia entre aguda y nueva: {:.4f}'.format(distancia3[-1,1]))

if distancia2[-1,1] < 0.8:
  print('La voz es grave')
elif distancia3[-1,1] < 0.4:
  print('La voz es de Ginna')

